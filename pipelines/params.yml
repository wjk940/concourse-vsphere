# Token used to download the product file from Pivotal Network. Find this
# on your Pivotal Network profile page:
# https://network.pivotal.io/users/dashboard/edit-profile
pivnet_token: <YOUR-PIVNET-TOKEN>

# The token used to download the repos from GitHub for the pipelines
github_token: <YOUR-GITHUB-TOKEN>

# Enter vCenter details
vcenter_host: <VCENTER-HOST-OR-IP>
vcenter_usr: <VCENTER-SERVICE-USR> # If user is tied to a domain, then escape the \, example `domain\\user`
vcenter_pwd: <VCENTER-SERVICE-USR-PWD>
vcenter_data_center: <VCENTER-DATA-CENTER-TO-DEPLOY>

# Ops Manager installation meta data
om_vm_host: <DESIRED-VCENTER-HOST-TO-DEPLOY-OM> # Optional - vCenter host to deploy Ops Manager on
om_data_store: <DATA-STORE-TO-DEPLOY-OPS-MANAGER> # vCenter datastore name to deploy Ops Manager on
om_fqdn: <DESIRED-OPS-MANAGER-FQDN> # FQDN to access Ops Manager, ex: opsmgr.example.com
om_usr: <DESIRED-OPS-MANAGER-USR> # Ops Manager admin username
om_pwd: <DESIRED-OPS-MANAGER-PWD> # Ops Manager admin password
om_ssh_pwd: <DESIRED-OPS-MANAGER-SSH-PWD> # Ops Manager ssh password to user ubuntu
om_decryption_pwd: <DESIRED-OPS-MANAGER-DECRYPTION-PWD> # Ops Manager decryption password
om_ntp_servers: <OPS-MANAGER-NTP-SERVERS> # Comma seperated list of NTP Servers
om_dns_servers: <OPS-MANAGER-DNS-SERVERS> # Comma seperated list of DNS Servers
om_gateway: <OPS-MANAGER-NETWORK-GATEWAY> # Ops Manager network gateway
om_netmask: <OPS-MANAGER-NETWORK-MASK> # Ops Manager network netmask
om_ip: <DESIRED-OPS-MANAGER-IP> # Ops Manager Static IP

om_vm_network: <OPS-MANAGER-NETWORK-NAME> # vCenter network name to use to deploy Ops Manager on
om_vm_name: <OPS-MANAGER-VM-NAME> # Ops Manager VM Name

# vCenter Cluster or Resource Pool to use to deploy Ops Manager.
# The format is
# - `/<Data Center Name>/host/<Cluster Name>/Resources/<Resource Pool Name>`
# - or `/<Data Center Name>/host/<Cluster Name>`
om_resource_pool: <OPS-MANAGER-RESOURCE-POOL-PATH>

om_disk_type: "thin" # Ops Manager VM Disk type, enter thick or thin
om_vm_power_state: true # Power on Ops Manager VM after creation. Possible values true/false

ephemeral_storage_names: <PCF-EPHEMERAL-STORAGE-NAMES> # Ephemeral Storage names from vCenter for use by PCF
persistent_storage_names: <PCF-PERSISTENT-STORAGE-NAMES> # Persistent Storage names from vCenter for use by PCF

trusted_certificates: # Trusted certificates to be deployed along with all VM's provisioned by BOSH
generate_vm_passwords: true # Allow BOSH to generate random passwords for each job. Possible values true or false
vm_disk_type: "thick" # BOSH provisioned VM Disk type, enter thick or thin

## AZ configuration for Ops Director
az_1_name: <az1-name> # AZ1 logical name. No spaces or special characters
az_1_cluster_name: <vcenter-az1-cluster-name> # vCenter Cluster name for AZ1
az_1_rp_name: <vcenter-az1-resource-pool-name> # vCenter Resource Pool name for AZ1

az_2_name: <az2-name> # AZ2 logical name. No spaces or special characters
az_2_cluster_name: <vcenter-az2-cluster-name> # vCenter Cluster name for AZ2
az_2_rp_name: <vcenter-az2-resource-pool-name> # vCenter Resource Pool name for AZ2

az_3_name: <az3-name> # AZ3 logical name. No spaces or special characters
az_3_cluster_name: <vcenter-az3-cluster-name> # vCenter Cluster name for AZ3
az_3_rp_name: <vcenter-az3-resource-pool-name> # vCenter Resource Pool name for AZ3

ntp_servers: # Comma Seperated NTP Servers for use for VM's deployed by BOSH
ops_dir_hostname: # Optional - Ops Director DNS resolvable name. Should be reachable from all networks
enable_vm_resurrector: true # Enable Bosh resurrector, possible values are true or false
max_threads: 30 # Max Threads count for VM deployment

## Network configuration for Ops Director
icmp_checks_enabled: true # Configure if ICMP checks are allowed, possible values true or false
infra_network_name: "INFRASTRUCTURE"
infra_vsphere_network: # vCenter Infrastructure network name
infra_nw_cidr: # Infrastructure network CIDR, ex: 10.0.0.0/22
infra_excluded_range: # Infrastructure network exclusion range
infra_nw_dns: # Infrastructure network DNS
infra_nw_gateway: # Infrastructure network Gateway
infra_nw_azs: # Comma seperated list of AZ's to be associated with this network

deployment_network_name: "DEPLOYMENT"
deployment_vsphere_network: # vCenter Deployment network name
deployment_nw_cidr: # Deployment network CIDR, ex: 10.0.0.0/22
deployment_excluded_range: # Deployment network exclusion range
deployment_nw_dns: # Deployment network DNS
deployment_nw_gateway: # Deployment network Gateway
deployment_nw_azs: # Comma seperated list of AZ's to be associated with this network

services_network_name: "SERVICES"
services_vsphere_network: # vCenter Services network name
services_nw_cidr: # Services network CIDR, ex: 10.0.0.0/22
services_excluded_range: # Services network exclusion range
services_nw_dns: # Services network DNS
services_nw_gateway: # Services network Gateway
services_nw_azs: # Comma seperated list of AZ's to be associated with this network

dynamic_services_network_name: "DYNAMIC-SERVICES"
dynamic_services_vsphere_network: # vCenter Dynamic Services network name
dynamic_services_nw_cidr: # Dynamic Services network CIDR, ex: 10.0.0.0/22
dynamic_services_excluded_range: # Dynamic Services network exclusion range
dynamic_services_nw_dns: # Dynamic Services network DNS
dynamic_services_nw_gateway: # Dynamic Services network Gateway
dynamic_services_nw_azs: # Comma seperated list of AZ's to be associated with this network

ert_singleton_job_az: # AZ to use for deployment of ERT Singleton jobs

loggregator_endpoint_port: # Loggegrator Port. Default is 443
company_name: # Company Name for Apps Manager

## Syslog endpoint configuration goes here
syslog_host:
syslog_port:
syslog_protocol:
enable_security_event_logging: true/false
syslog_drain_buffer_size: 10000

## Wildcard domain certs go here
ssl_termination_point: "external_ssl/external_non_ssl/haproxy"
ssl_cert:
ssl_private_key:

## Get this from the OPS Manager API docs for your release. Possible values are
## - /api/v0/certificates/generate (for 1.10)
## - /api/v0/rsa_certificates (for 1.9)
om_generate_ssl_endpoint:

disable_http_proxy: <true/false>
security_acknowledgement: "X"

## TCP routing and routing services
tcp_routing: <enable/disable>
tcp_routing_ports:
route_services: <enable/disable>
ignore_ssl_cert_verification: <true/false>
garden_network_pool_cidr: 10.254.0.0/22  ## Default value
garden_network_mtu: 1454  ## Default value

## SMTP configuration goes here
smtp_from:
smtp_address:
smtp_port:
smtp_user:
smtp_pwd:
smtp_auth_mechanism: # Possible values none, plain or cram-md5

## Authnetication type needed. For SAML please fork this repo and make the changes
authentication_mode: internal # Possible values internal or ldap

## LDAP Configuration goes here
ldap_url:
ldap_user:
ldap_pwd:
search_base:
search_filter:
group_search_base:
group_search_filter:
mail_attribute_name:
first_name_attribute:
last_name_attribute:

## Deployment domain names
system_domain:
apps_domain:

skip_cert_verify: false # Disable SSL certificate verification for this environment. Possible values true or false

disable_insecure_cookies: false # Disable insecure cookies on the Router. Possible values true or false

default_quota_memory_limit_mb: 10240  ## Default
default_quota_max_number_services: 1000  ## Default
allow_app_ssh_access: <true/false>

router_request_timeout_in_seconds: 900  ## Default

## Static IP's for the following jobs
ha_proxy_ips:
router_static_ips:
tcp_router_static_ips:
ssh_static_ips:

## Target email address to receive mysql monitor notifications
mysql_monitor_email: # Required - Enter a valid email address for the MySQL Monitor job
mysql_backups: "enable/disable"

## Default resource configuration
consul_server_instances: 1
nats_instances: 1
etcd_tls_server_instances: 1
nfs_server_instances: 1
mysql_proxy_instances: 1
mysql_instances: 1
backup_prepare_instances: 0
ccdb_instances: 0
uaadb_instances: 0
uaa_instances: 1
cloud_controller_instances: 1
ha_proxy_instances: 0
router_instances: 1
mysql_monitor_instances: 1
clock_global_instances: 1
cloud_controller_worker_instances: 1
diego_database_instances: 1
diego_brain_instances: 1
diego_cell_instances: 3
doppler_instances: 1
loggregator_traffic_controller_instances: 1
tcp_router_instances: 1

## JMX Bridge tile configuration goes here
jmx_admin_usr:
jmx_admin_pwd:
jmx_security_logging: <true/false>
jmx_use_ssl: <true/false>
